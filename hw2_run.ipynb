{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(pos_train,neg_train,vocab,pos_test,neg_test,alpha=10**-5):\n",
    "\t'''\n",
    "\tFunction to perform naive baiyes using standard equation.\n",
    "\tpos_train - positive instances in training set,\n",
    "\tneg_train - negative instances in training set,\n",
    "\tvocab - vocab\n",
    "\tpos_test - positive instances in test set,\n",
    "\tneg_test - negative incstances in test set,\n",
    "\talpha - alpha value for laplace smoothing,\n",
    "\treturns: tp,fp,fn,tn\n",
    "\t'''\n",
    "\n",
    "\t#get counts of each word in classes\n",
    "\tpos_counts = Counter(chain(*pos_train))\n",
    "\tneg_counts = Counter(chain(*neg_train))\n",
    "\ttotal_pos = sum(pos_counts[key] for key in pos_counts.keys())\n",
    "\ttotal_neg = sum(neg_counts[key] for key in neg_counts.keys())\n",
    "\n",
    "\tnum_docs = len(pos_train) + len(neg_train)\n",
    "\tprop_pos = len(pos_train)/num_docs\n",
    "\tprop_neg = len(neg_train)/num_docs\n",
    "\t\n",
    "\t#calculate true pos, false pos, false neg, true neg for standard\n",
    "\ttp,fp,fn,tn= [0]*4\n",
    "\n",
    "\tfor doc in pos_test:\n",
    "\t\tis_pos = prop_pos\n",
    "\t\tis_neg = prop_neg\n",
    "\n",
    "\t\tfor word in doc:\n",
    "\t\t\tprob_word_pos = (pos_counts[word] + alpha)/(total_pos + alpha*len(vocab))\n",
    "\t\t\tprob_word_neg = (neg_counts[word] + alpha)/(total_neg + alpha*len(vocab))\n",
    "\t\t\tis_pos *= prob_word_pos\n",
    "\t\t\tis_neg *= prob_word_neg\n",
    "\n",
    "\t\tif is_pos > is_neg:\n",
    "\t\t\ttp += 1\n",
    "\t\telif is_neg > is_pos:\n",
    "\t\t\tfn += 1\n",
    "\t\t#if probabilities are equal, flip a coin\n",
    "\t\telse:\n",
    "\t\t\tif random.random() > 0.5:\n",
    "\t\t\t\ttp += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfn += 1\n",
    "\n",
    "\tfor doc in neg_test:\n",
    "\t\tis_pos = prop_pos\n",
    "\t\tis_neg = prop_neg\n",
    "\n",
    "\t\tfor word in doc:\n",
    "\t\t\tprob_word_pos = (pos_counts[word] + alpha)/(total_pos + alpha*len(vocab))\n",
    "\t\t\tprob_word_neg = (neg_counts[word] + alpha)/(total_neg + alpha*len(vocab))\n",
    "\t\t\tis_pos *= prob_word_pos\n",
    "\t\t\tis_neg *= prob_word_neg\n",
    "\n",
    "\t\tif is_pos > is_neg:\n",
    "\t\t\tfp += 1\n",
    "\t\telif is_neg > is_pos:\n",
    "\t\t\ttn += 1\n",
    "\t\t#if probabilities are equal, flip a coin\n",
    "\t\telse:\n",
    "\t\t\tif random.random() > 0.5:\n",
    "\t\t\t\ttn += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfp += 1\n",
    "\n",
    "\treturn tp,fp,fn,tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=10**-5):\n",
    "\t'''\n",
    "\tFunction to perform naive baiyes using log probabilities.\n",
    "\tpos_train - positive instances in training set,\n",
    "\tneg_train - negative instances in training set,\n",
    "\tvocab - vocab\n",
    "\tpos_test - positive instances in test set,\n",
    "\tneg_test - negative incstances in test set,\n",
    "\talpha - alpha value for laplace smoothing,\n",
    "\treturns: tp_log,fp_log,fn_log,tn_log\n",
    "\t'''\n",
    "\n",
    "\t#get counts of each word in classes\n",
    "\tpos_counts = Counter(chain(*pos_train))\n",
    "\tneg_counts = Counter(chain(*neg_train))\n",
    "\ttotal_pos = sum(pos_counts[key] for key in pos_counts.keys())\n",
    "\ttotal_neg = sum(neg_counts[key] for key in neg_counts.keys())\n",
    "\n",
    "\tnum_docs = len(pos_train) + len(neg_train)\n",
    "\tprop_pos = len(pos_train)/num_docs\n",
    "\tprop_neg = len(neg_train)/num_docs\n",
    "\t\n",
    "\t#calculate true pos, false pos, false neg, true neg\n",
    "\ttp_log,fp_log,fn_log,tn_log = [0]*4\n",
    "\n",
    "\tfor doc in pos_test:\n",
    "\t\tpos_log = math.log(prop_pos)\n",
    "\t\tneg_log = math.log(prop_neg)\n",
    "\n",
    "\t\tfor word in doc:\n",
    "\t\t\tpos_log += math.log((pos_counts[word] + alpha)/(total_pos + alpha*len(vocab)))\n",
    "\t\t\tneg_log += math.log((neg_counts[word] + alpha)/(total_neg + alpha*len(vocab)))\n",
    "\n",
    "\t\tif pos_log > neg_log:\n",
    "\t\t\ttp_log += 1\n",
    "\t\telif neg_log > pos_log:\n",
    "\t\t\tfn_log += 1\n",
    "\t\t#if probabilities are equal, flip a coin\n",
    "\t\telse:\n",
    "\t\t\tif random.random() > 0.5:\n",
    "\t\t\t\ttp_log += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfn_log += 1\n",
    "\n",
    "\tfor doc in neg_test:\n",
    "\t\tpos_log = math.log(prop_pos)\n",
    "\t\tneg_log = math.log(prop_neg)\n",
    "\n",
    "\t\tfor word in doc:\n",
    "\t\t\tprob_word_pos = (pos_counts[word] + alpha)/(total_pos + alpha*len(vocab))\n",
    "\t\t\tprob_word_neg = (neg_counts[word] + alpha)/(total_neg + alpha*len(vocab))\n",
    "\t\t\tpos_log += math.log(prob_word_pos)\n",
    "\t\t\tneg_log += math.log(prob_word_neg)\n",
    "\n",
    "\t\tif pos_log > neg_log:\n",
    "\t\t\tfp_log += 1\n",
    "\t\telif neg_log > pos_log:\n",
    "\t\t\ttn_log += 1\n",
    "\t\t#if probabilities are equal, flip a coin\n",
    "\t\telse:\n",
    "\t\t\tif random.random() > 0.5:\n",
    "\t\t\t\ttn_log += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfp_log += 1\n",
    "\n",
    "\treturn tp_log,fp_log,fn_log,tn_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.1 (18 Points)** First, perform the classification of the\n",
    "instances in the test set by comparing posterior probabilities, Pr(yi\n",
    "| Doc), according to Eq. (1), for\n",
    "both classes. Then, report (i) the accuracy of your model; (ii) its precision; (iii) its recall; and (iv)\n",
    "the confusion matrix resulting from this experiment. \n",
    "\n",
    ">**NOTE: For this question I implemented laplace smoothing with alpha = 1e-5 as to avoid dealing with 0 probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 2426\n",
      "Number of negative training instances: 2407\n",
      "Number of positive test instances: 2498\n",
      "Number of negative test instances: 2565\n",
      "Vocabulary (training set): 42195\n",
      "\n",
      "Results for Standard Equation\n",
      "Precision: 0.5955480890382192\n",
      "Recall: 0.567654123298639\n",
      "Accuracy: 0.5964842978471262\n",
      "Confusion Matrix: \n",
      "TP: 1418 FN: 1080\n",
      "FP: 963 TN: 1602\n"
     ]
    }
   ],
   "source": [
    "(pos_train, neg_train, vocab) = load_training_set(0.2, 0.2)\n",
    "(pos_test,  neg_test)         = load_test_set(0.2, 0.2)\n",
    "tp,fp,fn,tn = naive_bayes(pos_train,neg_train,vocab,pos_test,neg_test)\n",
    "\n",
    "print(\"Number of positive training instances:\", len(pos_train))\n",
    "print(\"Number of negative training instances:\", len(neg_train))\n",
    "print(\"Number of positive test instances:\", len(pos_test))\n",
    "print(\"Number of negative test instances:\", len(neg_test))\n",
    "with open('vocab.txt','w',encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(\"%s\\n\" % word)\n",
    "print(\"Vocabulary (training set):\", len(vocab))\n",
    "print()\n",
    "print(\"Results for Standard Equation\")\n",
    "print(f\"Precision: {(tp/(tp+fp))}\")\n",
    "print(f\"Recall: {(tp/(tp+fn))}\")\n",
    "print(f\"Accuracy: {((tp+tn)/(tp+fp+tn+fn))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp} FN: {fn}\")\n",
    "print(f\"FP: {fp} TN: {tn}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same experiment above but classify the instances in the test set by comparing log-probabilities, log(Pr(yi| Doc)), according to Eq. (5), for both classes. Report the same quantities as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Log Transform Equation\n",
      "Precision: 0.7220367278797997\n",
      "Recall: 0.6925540432345877\n",
      "Accuracy: 0.7167687142010666\n",
      "Confusion Matrix: \n",
      "TP: 1730 FN: 768\n",
      "FP: 666 TN: 1899\n"
     ]
    }
   ],
   "source": [
    "tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test)\n",
    "print(\"Results for Log Transform Equation\")\n",
    "print(f\"Precision: {(tp_log/(tp_log+fp_log))}\")\n",
    "print(f\"Recall: {(tp_log/(tp_log+fn_log))}\")\n",
    "print(f\"Accuracy: {((tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp_log} FN: {fn_log}\")\n",
    "print(f\"FP: {fp_log} TN: {tn_log}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss whether classifying instances by computing log-probabilities, instead of probabilities, affects the modelâ€™s performance. Assuming that this transformation does have an impact on performance, does it affect more strongly the modelâ€™s accuracy, precision, or recall? Why do you think that is the case?\n",
    "\n",
    ">Classifying instances by computing log-probabilities significantly improves the model's performance when compared to the standard probabilities. It has the strongest effect on recall (22% increase), however the effect is almost equally strong for precision (21.14%) and accuracy (20.3%). When comparing the confusion matrices for both, it is shown that computing log-probabilities increases the number of true positives and true negatives by ~20%. The reason for the improved performance is likely due to the fact that computing log-probabilities changes the comparisson to a summation rather than a product. Since I used a low alpha (1e-5), it is likely that documents including words that were not encountered in the training set had extremely low scores resulting in false positives and false negatives.\n",
    "\n",
    "**Q.2 (18 Points)** In this experiment, you should use 20% of the training set and 20% of the test set; i.e., call the\n",
    "dataset-loading functions by passing 0.2 as their parameters. You should first report the confusion\n",
    "matrix, precision, recall, and accuracy of your classifier (when evaluated on the test set) when using\n",
    "Î± = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 2525\n",
      "Number of negative training instances: 2504\n",
      "Number of positive test instances: 2520\n",
      "Number of negative test instances: 2441\n",
      "Vocabulary (training set): 42957\n",
      "\n",
      "Results for Log Transform Equation Using Alpha = 1\n",
      "Precision: 0.8605683836589698\n",
      "Recall: 0.7690476190476191\n",
      "Accuracy: 0.8193912517637573\n",
      "Confusion Matrix: \n",
      "TP: 1938 FN: 582\n",
      "FP: 314 TN: 2127\n"
     ]
    }
   ],
   "source": [
    "(pos_train, neg_train, vocab) = load_training_set(0.2, 0.2)\n",
    "(pos_test,  neg_test)         = load_test_set(0.2, 0.2)\n",
    "tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=1)\n",
    "\n",
    "print(\"Number of positive training instances:\", len(pos_train))\n",
    "print(\"Number of negative training instances:\", len(neg_train))\n",
    "print(\"Number of positive test instances:\", len(pos_test))\n",
    "print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "with open('vocab.txt','w',encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(\"%s\\n\" % word)\n",
    "print(\"Vocabulary (training set):\", len(vocab))\n",
    "print()\n",
    "\n",
    "print(\"Results for Log Transform Equation Using Alpha = 1\")\n",
    "print(f\"Precision: {(tp_log/(tp_log+fp_log))}\")\n",
    "print(f\"Recall: {(tp_log/(tp_log+fn_log))}\")\n",
    "print(f\"Accuracy: {((tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp_log} FN: {fn_log}\")\n",
    "print(f\"FP: {fp_log} TN: {tn_log}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, vary the value of Î± from 0.0001 to 1000, by multiplying Î± with 10 each time. That is, try\n",
    "values of Î± equal to 0.0001, 0.001, 0.01, 0.1, 1.0, 100, and 1000. For each value, record the accuracy of\n",
    "the resulting model when evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Alpha = 0.0001: 0.7266680104817577\n",
      "Accuracy for Alpha = 0.001: 0.7478330981656924\n",
      "Accuracy for Alpha = 0.01: 0.7718201975408184\n",
      "Accuracy for Alpha = 0.1: 0.7943962910703487\n",
      "Accuracy for Alpha = 1.0: 0.8193912517637573\n",
      "Accuracy for Alpha = 10.0: 0.8383390445474702\n",
      "Accuracy for Alpha = 100.0: 0.8322918766377746\n",
      "Accuracy for Alpha = 1000.0: 0.7851239669421488\n"
     ]
    }
   ],
   "source": [
    "i = 0.0001\n",
    "alpha_vals = []\n",
    "accuracy = []\n",
    "while i <= 1000:\n",
    "    tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=i)\n",
    "    alpha_vals.append(i)\n",
    "    acc_i = (tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log)\n",
    "    accuracy.append(acc_i)\n",
    "    print(f\"Accuracy for Alpha = {i}: {acc_i}\")\n",
    "    i *= 10\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a plot of the modelâ€™s accuracy on the\n",
    "test set (shown on the y-axis) as a function of the value of Î± (shown on the x-axis). The x-axis should\n",
    "represent Î± values and use a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xscale(\"log\")\n",
    "plt.plot(alpha_vals,accuracy)\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Alpha\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze this graph and discuss why do you think the accuracy suffers when Î± is too high or too low.\n",
    "\n",
    ">This graph shows that accuracy increases as alpha increases until reaching its maximum accuracy at alpha = 10. For alpha values > 10, accuracy decreases as alpha increases. Alpha values which are too high or too low result in lower accuracy due to the fact that tweaking the value of alpha is equivalent to adding fake occurences of words in the training set. If alpha is too low, then the model will be too biased towards words that occur in the training data causing it to overfit. If alpha is too high, the model may overcompensate by increasing it's percieved frequency of all words; if a word occurs 10 times in the positive training set but 0 times in the negative, adding 10,000 fake instances that word could discount the fact that the word only occured in the positive training set.\n",
    "\n",
    "**Q.3 (18 Points)** Now you will investigate the impact of the training set size on the performance of the model. The classification of new instances, here, should be done by comparing the posterior log-probabilities, log(Pr(yi | Doc)) according to Eq. (5), for both classes. You should use the value of Î± that resulted in the highest accuracy according to your experiments in the previous question. In this question, you should use 100% of the training set and 100% of the test set; i.e., call the dataset-loading functions by passing 1.0 as their parameters. Then, report (i) the accuracy of your model; (ii) its precision; (iii) its recall; and (iv) the confusion matrix resulting from this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 12500\n",
      "Number of negative training instances: 12500\n",
      "Number of positive test instances: 12500\n",
      "Number of negative test instances: 12500\n",
      "Vocabulary (training set): 92603\n",
      "\n",
      "Results for Log Transform Equation Using Alpha = 10\n",
      "Precision: 0.8686214442013129\n",
      "Recall: 0.79392\n",
      "Accuracy: 0.83692\n",
      "Confusion Matrix: \n",
      "TP: 9924 FN: 2576\n",
      "FP: 1501 TN: 10999\n"
     ]
    }
   ],
   "source": [
    "(pos_train, neg_train, vocab) = load_training_set(1, 1)\n",
    "(pos_test,  neg_test)         = load_test_set(1, 1)\n",
    "tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=10)\n",
    "\n",
    "print(\"Number of positive training instances:\", len(pos_train))\n",
    "print(\"Number of negative training instances:\", len(neg_train))\n",
    "print(\"Number of positive test instances:\", len(pos_test))\n",
    "print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "with open('vocab.txt','w',encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(\"%s\\n\" % word)\n",
    "print(\"Vocabulary (training set):\", len(vocab))\n",
    "print()\n",
    "print(\"Results for Log Transform Equation Using Alpha = 10\")\n",
    "print(f\"Precision: {(tp_log/(tp_log+fp_log))}\")\n",
    "print(f\"Recall: {(tp_log/(tp_log+fn_log))}\")\n",
    "print(f\"Accuracy: {((tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp_log} FN: {fn_log}\")\n",
    "print(f\"FP: {fp_log} TN: {tn_log}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.4 (18 Points)** Now repeat the experiment above but use only 50% of the training instances; that\n",
    "is, load the training set by calling load training set(0.5, 0.5). The entire test set should be\n",
    "used. Report the same quantities as in the previous question. Discuss whether using such a smaller\n",
    "training set had any impact on the performance of your learned model. Analyze the confusion matrices\n",
    "(of this question and the previous one) and discuss whether one particular class was more affected by\n",
    "changing the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 6262\n",
      "Number of negative training instances: 6344\n",
      "Number of positive test instances: 12500\n",
      "Number of negative test instances: 12500\n",
      "Vocabulary (training set): 67067\n",
      "\n",
      "Results for Log Transform Equation Using Alpha = 10\n",
      "Precision: 0.8689846760462407\n",
      "Recall: 0.77576\n",
      "Accuracy: 0.8294\n",
      "Confusion Matrix: \n",
      "TP: 9697 FN: 2803\n",
      "FP: 1462 TN: 11038\n"
     ]
    }
   ],
   "source": [
    "(pos_train, neg_train, vocab) = load_training_set(0.5, 0.5)\n",
    "tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=10)\n",
    "\n",
    "print(\"Number of positive training instances:\", len(pos_train))\n",
    "print(\"Number of negative training instances:\", len(neg_train))\n",
    "print(\"Number of positive test instances:\", len(pos_test))\n",
    "print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "with open('vocab.txt','w',encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(\"%s\\n\" % word)\n",
    "print(\"Vocabulary (training set):\", len(vocab))\n",
    "print()\n",
    "print(\"Results for Log Transform Equation Using Alpha = 10\")\n",
    "print(f\"Precision: {(tp_log/(tp_log+fp_log))}\")\n",
    "print(f\"Recall: {(tp_log/(tp_log+fn_log))}\")\n",
    "print(f\"Accuracy: {((tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp_log} FN: {fn_log}\")\n",
    "print(f\"FP: {fp_log} TN: {tn_log}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using only 50% of the training instances resulted in worse recall and accuracy while precision was roughly the same. By comparing the confusion matrices, we can see that using 50% of the training data reulted in a higher number of false negatives and true negatives, while yielding a lower amount of true positives and false positives. In this case, using 50% of the training set seemed to incur a bias towards negative reviews. This is likely due to the fact that using 50% of the training instances resulted in 6262 positive training instances and 6344 negative training instances.\n",
    "\n",
    "**Q.5 (10 Points)** In this application (i.e., accurately classifying movie reviews), would you say that it\n",
    "is more important to have high accuracy, high precision, or high recall? Justify your opinion\n",
    "\n",
    ">Since we are analyzing many movie reviews, we are likely trying to guage public opinion of a given movie. In this case, having a high recall is most important as we care about accurately identifying wether a movie has more positive or more negative reviews. If the model has a high recall, then we can assume that the number of documents classified as positive is close to the actual number of positive reviews (given our accuracy and precision is not horrible). With this information we can determine wether or not the majority of reviews favor the movie. That being said, a model could achieve a recall of 1 just by classifying every review as positive. Because of this, it is still important for a model to have high accuracy and precision.\n",
    "\n",
    "**Q.6 (18 Points)** You should use the value of Î± that resulted in the highest accuracy according to your experiments in the previous questions. You will now conduct an experiment where you use only 10% of the available positive training instances and that uses 50% of the available negative training instances. That is, use load training set(0.1, 0.5). The entire test set should be used. Show the confusion matrix of your trained model, as well as\n",
    "its accuracy, precision, and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 1287\n",
      "Number of negative training instances: 6198\n",
      "Number of positive test instances: 12500\n",
      "Number of negative test instances: 12500\n",
      "Vocabulary (training set): 50530\n",
      "\n",
      "Results for Unbalanced Training Using Alpha = 10\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.00048\n",
      "Accuracy: 0.5002\n",
      "Confusion Matrix: \n",
      "TP: 6 FN: 12494\n",
      "FP: 1 TN: 12499\n"
     ]
    }
   ],
   "source": [
    "(pos_train, neg_train, vocab) = load_training_set(0.1, 0.5)\n",
    "tp_log,fp_log,fn_log,tn_log = naive_bayes_log(pos_train,neg_train,vocab,pos_test,neg_test,alpha=10)\n",
    "\n",
    "print(\"Number of positive training instances:\", len(pos_train))\n",
    "print(\"Number of negative training instances:\", len(neg_train))\n",
    "print(\"Number of positive test instances:\", len(pos_test))\n",
    "print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "with open('vocab.txt','w',encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(\"%s\\n\" % word)\n",
    "print(\"Vocabulary (training set):\", len(vocab))\n",
    "print()\n",
    "print(\"Results for Unbalanced Training Using Alpha = 10\")\n",
    "print(f\"Precision: {(tp_log/(tp_log+fp_log))}\")\n",
    "print(f\"Recall: {(tp_log/(tp_log+fn_log))}\")\n",
    "print(f\"Accuracy: {((tp_log+tn_log)/(tp_log+fp_log+tn_log+fn_log))}\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(f\"TP: {tp_log} FN: {fn_log}\")\n",
    "print(f\"FP: {fp_log} TN: {tn_log}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this modelâ€™s performance to the performance (according to these same metrics) of the model trained in question Q.4â€”that is, a model that was trained under a balanced dataset. Discuss how training under an unbalanced dataset affected each of these performance metrics.\n",
    "\n",
    ">The performance of the unbalanced training set is significantly worse than the balanced training set. The unbalanced training set yields a pitifully low recall of almost 0 and extremely low accuracy of 0.5, compared to 0.78 and 0.83 respectively with the balanced training set. Despite this, the unabalanced training set yielded a similar precision to the balanced training set. Looking at the confusion matrix corresponding to the unbalanced training data, we can see that only 7 of the 25,000 documents were classified as positive. This is due to the fact that the negative training set was 5 times larger than the positive training set, causing the model to classify almost every document as negative due to the inflated word frequencies for the negative dataset. Since the model barely classified any instances as positive, we would expect a near 0 recall. An accuracy of 0.5 is also expected since half of the testing documents are in fact negative. Since the model almost exclusively labeled instances as negative, it is not surprising that an accuracy of 0.86 was recorded since the documents labeled as positive must have had to be significantly similar to the positive training and/or dissimilar to the negative training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
